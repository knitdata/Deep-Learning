---
author: ' Min Tamang'
date: ' 05/02/2020'
output:
  pdf_document: default
  html_notebook: default
---


# One-hot encoding of words and characters

```{r}
#word-level one-hot encoding(toy example)
# initial sample data in sentences
samples <- c("The cat sat on the mat.", "The dog ate my homework.")
  
# First, build an index of all tokens in the data.
token_index <- list()
for (sample in samples)
  #tokenize the samples 
  for (word in strsplit(sample, " ")[[1]])
    if (!word %in% names(token_index))
      # assigns a unique index to each unique word
      token_index[[word]] <- length(token_index) + 2 

# vectorizes the samples.
max_length <- 10

# store the results
results <- array(0, dim = c(length(samples), 
                            max_length, 
                            max(as.integer(token_index))))

for (i in 1:length(samples)) {
  sample <- samples[[i]]
  words <- head(strsplit(sample, " ")[[1]], n = max_length)
  for (j in 1:length(words)) {
    index <- token_index[[words[[j]]]]
    results[[i, j, index]] <- 1
  }
}
```

```{r}
#charater-level one-hot encoding(toy example)
samples <- c("The cat sat on the mat.", "The dog ate my homework.")

ascii_tokens <- c("", sapply(as.raw(c(32:126)), rawToChar))
token_index <- c(1:(length(ascii_tokens)))
names(token_index) <- ascii_tokens

max_length <- 50

results <- array(0, dim = c(length(samples), max_length, length(token_index)))

for (i in 1:length(samples)) {
  sample <- samples[[i]]
  characters <- strsplit(sample, "")[[1]]
  for (j in 1:length(characters)) {
    character <- characters[[j]]
    results[i, j, token_index[[character]]] <- 1
  }
}
```

```{r}
# Using Keras for word-leve one-hot encoding
library(keras)

samples <- c("The cat sat on the mat.", "The dog ate my homework.")

# Create a tokenizer of 1000 most common words
tokenizer <- text_tokenizer(num_words = 1000) %>%
  fit_text_tokenizer(samples) # build words index

# Turn strings into lists of integer indices
sequences <- texts_to_sequences(tokenizer, samples)

# one-hot binary representation
one_hot_results <- texts_to_matrix(tokenizer, samples, mode = "binary")

# recover computed word index
word_index <- tokenizer$word_index

cat("Found", length(word_index), "unique tokens.\n")
```

```{r}
#word-level one-hot encoding with hashing trick(toy example)
library(hashFunction)

samples <- c("The cat sat on the mat.", "The dog ate my homework.")
# store the words as vectors of sixe 1,000
dimensionality <- 1000
max_length <- 10

results <- array(0, dim = c(length(samples), max_length, dimensionality))

for (i in 1:length(samples)) {
  sample <- samples[[i]]
  words <- head(strsplit(sample, " ")[[1]], n = max_length)
  for (j in 1:length(words)) {
#hash the word into a random integer
    index <- abs(spooky.32(words[[i]])) %% dimensionality
    results[[i, j, index]] <- 1
  }
}
```


# Using word embeddings
 
 **Learning word embeddings with an embedding layer**
```{r}
#intantiating an embeding layer
embedding_layer <- layer_embedding(input_dim = 1000, output_dim = 64) 
```


```{r}
#load the IMDB dataset for use with an embedding layer
# Number of words to consider as features
max_features <- 10000
# Cut texts after this number of words 
maxlen <- 20

# Load the data as lists of integers
imdb <- dataset_imdb(num_words = max_features)
c(c(x_train, y_train), c(x_test, y_test)) %<-% imdb

# Turns lists of integers into a 2D integer tensor of shape `(samples, maxlen)`
x_train <- pad_sequences(x_train, maxlen = maxlen)
x_test <- pad_sequences(x_test, maxlen = maxlen)
```

```{r}
#Using an embedding layer and classifier on the IMDB data
model <- keras_model_sequential() %>% 
# spefify the max input length to the embedding layer
  layer_embedding(input_dim = 10000, output_dim = 8, 
                  input_length = maxlen) %>% 
  
# flatten th 3D tensor of embeddings into a 2D
  layer_flatten() %>% 
# add the classifier on top
  layer_dense(units = 1, activation = "sigmoid") 

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history <- model %>% fit(
  x_train, y_train,
  epochs = 10,
  batch_size = 32,
  validation_split = 0.2
)
```


**Putting it all together: from raw text to word embeddings**


```{r}
# processing the labels of the raw IMDB data
imdb_dir <- "~/Documents/STAT /STAT 654/aclImdb"
train_dir <- file.path(imdb_dir, "train")

labels <- c()
texts <- c()

for (label_type in c("neg", "pos")) {
  label <- switch(label_type, neg = 0, pos = 1)
  dir_name <- file.path(train_dir, label_type)
  for (fname in list.files(dir_name, pattern = glob2rx("*.txt"), 
                           full.names = TRUE)) {
    texts <- c(texts, readChar(fname, file.info(fname)$size))
    labels <- c(labels, label)
  }
}
```

**Tokenize the data**

```{r}
maxlen <- 100                 # cut reviews after 100 words
training_samples <- 200       # train on 200 samples
validation_samples <- 10000   # validate on 10000 samples
max_words <- 10000            # consider top 10,000 words in the dataset

tokenizer <- text_tokenizer(num_words = max_words) %>% 
  fit_text_tokenizer(texts)

sequences <- texts_to_sequences(tokenizer, texts)

word_index = tokenizer$word_index
cat("Found", length(word_index), "unique tokens.\n")

data <- pad_sequences(sequences, maxlen = maxlen)

labels <- as.array(labels)
cat("Shape of data tensor:", dim(data), "\n")
cat('Shape of label tensor:', dim(labels), "\n")

# Split the data into a training set and a validation set
indices <- sample(1:nrow(data))
training_indices <- indices[1:training_samples]
validation_indices <- indices[(training_samples + 1): 
                              (training_samples + validation_samples)]

x_train <- data[training_indices,]
y_train <- labels[training_indices]

x_val <- data[validation_indices,]
y_val <- labels[validation_indices]
```

**Download the GloVe word embeddings**

```{r}
# parsing the GloVe word-embeding file
glove_dir <- '~/Documents/STAT /STAT 654/glove.6B'
lines <- readLines( file.path(glove_dir,"glove.6B.100d.txt"))

embeddings_index <- new.env(hash = TRUE, parent = emptyenv())
for (i in 1:length(lines)) {
  line <- lines[[i]]
  values <- strsplit(line, " ")[[1]]
  word <- values[[1]]
  embeddings_index[[word]] <- as.double(values[-1])
}

cat("Found", length(embeddings_index), "word vectors.\n")
```


```{r}
# prepare the GloVe word-embeddings matrix
embedding_dim <- 100

embedding_matrix <- array(0, c(max_words, embedding_dim))

for (word in names(word_index)) {
  index <- word_index[[word]]
  if (index < max_words) {
    embedding_vector <- embeddings_index[[word]]
    if (!is.null(embedding_vector))
#Words not found in the embedding index will be all zeros
      embedding_matrix[index+1,] <- embedding_vector
  }
}
```

**Define a model**
```{r}
model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_words, output_dim = embedding_dim, 
                  input_length = maxlen) %>% 
  layer_flatten() %>% 
  layer_dense(units = 32, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

summary(model)
```

**Load the GloVe embeddings in the model**
```{r}
get_layer(model, index = 1) %>% 
  set_weights(list(embedding_matrix)) %>% 
  freeze_weights()
```

**Train and evaluate the model**

```{r}
#compile and train the model
model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history <- model %>% fit(
  x_train, y_train,
  epochs = 10,
  batch_size = 32,
  validation_data = list(x_val, y_val)
)

save_model_weights_hdf5(model, "pre_trained_glove_model.h5")
```

```{r}
# plot the results
plot(history)
```


```{r}
#train the same model without pretrained word embeddings
model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_words, output_dim = embedding_dim, 
                  input_length = maxlen) %>% 
  layer_flatten() %>% 
  layer_dense(units = 32, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history <- model %>% fit(
  x_train, y_train,
  epochs = 10,
  batch_size = 32,
  validation_data = list(x_val, y_val)
)
```

```{r}
plot(history)
```



**evaluate the model on the test data**

```{r}
#tokenize the data of the test set
test_dir <- file.path(imdb_dir, "test")

labels <- c()
texts <- c()

for (label_type in c("neg", "pos")) {
  label <- switch(label_type, neg = 0, pos = 1)
  dir_name <- file.path(test_dir, label_type)
  for (fname in list.files(dir_name, pattern = glob2rx("*.txt"), 
                           full.names = TRUE)) {
    texts <- c(texts, readChar(fname, file.info(fname)$size))
    labels <- c(labels, label)
  }
}

sequences <- texts_to_sequences(tokenizer, texts)
x_test <- pad_sequences(sequences, maxlen = maxlen)
y_test <- as.array(labels)
```


```{r}
#evaluate model on the test set
model %>% 
  load_model_weights_hdf5("pre_trained_glove_model.h5") %>% 
  evaluate(x_test, y_test, verbose = 0)
```


**Understanding recurrent neural networks**

**first recurrent layer in Keras**

```{r}
layer_simple_rnn(units = 32)
```


```{r}
# last state sequence
model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = 10000, output_dim = 32) %>% 
  layer_simple_rnn(units = 32)

summary(model)
```

```{r}
# full state sequence
model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = 10000, output_dim = 32) %>% 
  layer_simple_rnn(units = 32, return_sequences = TRUE)

summary(model)
```


```{r}
# stack up several recurrent layers
model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = 10000, output_dim = 32) %>% 
  layer_simple_rnn(units = 32, return_sequences = TRUE) %>% 
  layer_simple_rnn(units = 32, return_sequences = TRUE) %>%
  layer_simple_rnn(units = 32, return_sequences = TRUE) %>%
  layer_simple_rnn(units = 32)  # the last outputs.

summary(model)
```


```{r}
# use model on IMDB dataset
max_features <- 10000  # Number of words to consider as features
maxlen <- 500  # Cuts off texts after this many words 
batch_size <- 32

cat("Loading data...\n")
imdb <- dataset_imdb(num_words = max_features)
c(c(input_train, y_train), c(input_test, y_test)) %<-% imdb 
cat(length(input_train), "train sequences\n")
cat(length(input_test), "test sequences")

cat("Pad sequences (samples x time)\n")
input_train <- pad_sequences(input_train, maxlen = maxlen)
input_test <- pad_sequences(input_test, maxlen = maxlen)
cat("input_train shape:", dim(input_train), "\n")
cat("input_test shape:", dim(input_test), "\n")
```

```{r}
# train the model with embedding and simple RNN layers
model <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_features, output_dim = 32) %>%
  layer_simple_rnn(units = 32) %>%
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history <- model %>% fit(
  input_train, y_train,
  epochs = 10,
  batch_size = 128,
  validation_split = 0.2
)
```


```{r}
plot(history)
```



**Concrete LSTM example in Keras**

```{r}
# using the LSTM layer in Keras
model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_features, output_dim = 32) %>% 
  layer_lstm(units = 32) %>% 
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = "rmsprop", 
  loss = "binary_crossentropy", 
  metrics = c("acc")
)

history <- model %>% fit(
  input_train, y_train,
  epochs = 10,
  batch_size = 128,
  validation_split = 0.2
)
```

```{r}
plot(history)
```



**Advance use of recurrent neural networks**


**A temperature forecasting problem**

```{r}
dir.create("~/Documents/STAT /STAT 654/jena_climate", recursive = TRUE)
download.file(
  "https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip",
  "~/Documents/STAT /STAT 654/jena_climate/jena_climate_2009_2016.csv.zip"
)
unzip(
  "~/Documents/STAT /STAT 654/jena_climate/jena_climate_2009_2016.csv.zip",
  exdir = "~/Documents/STAT /STAT 654/jena_climate"
)
```

```{r}
# inspect the data of the Jena weather dataset
library(tibble)
library(readr)

data_dir <- "~/Documents/STAT /STAT 654/jena_climate"
fname <- file.path(data_dir, "jena_climate_2009_2016.csv")
data <- read_csv(fname)
```

```{r}
glimpse(data)
```


```{r}
# plot the temperature timeseries
library(ggplot2)
ggplot(data, aes(x = 1:nrow(data), y = `T (degC)`)) + geom_line()
```



```{r}
# plot temperature of first 10 days
ggplot(data[1:1440,], aes(x = 1:1440, y = `T (degC)`)) + geom_line()
```


**Preparing the data**

```{r}
#covert the data to a floating point matrix
data <- data.matrix(data[,-1])
```


```{r}
# normalize the data
train_data <- data[1:200000,]
mean <- apply(train_data, 2, mean)
std <- apply(train_data, 2, sd)
data <- scale(data, center = mean, scale = std)
```


```{r}
# generator yielding timeseries samples and their targets
generator <- function(data, lookback, delay, min_index, max_index,
                      shuffle = FALSE, batch_size = 128, step = 6) {
  if (is.null(max_index))
    max_index <- nrow(data) - delay - 1
  i <- min_index + lookback
  function() {
    if (shuffle) {
      rows <- sample(c((min_index+lookback):max_index), size = batch_size)
    } else {
      if (i + batch_size >= max_index)
        i <<- min_index + lookback
      rows <- c(i:min(i+batch_size-1, max_index))
      i <<- i + length(rows)
    }
    
    samples <- array(0, dim = c(length(rows), 
                                lookback / step,
                                dim(data)[[-1]]))
    targets <- array(0, dim = c(length(rows)))
                     
    for (j in 1:length(rows)) {
      indices <- seq(rows[[j]] - lookback, rows[[j]] - 1, 
                     length.out = dim(samples)[[2]])
      samples[j,,] <- data[indices,]
      targets[[j]] <- data[rows[[j]] + delay,2]
    }            
    
    list(samples, targets)
  }
}
```


```{r}
#prepare the training, validation and test generators
lookback <- 1440
step <- 6
delay <- 144
batch_size <- 128

train_gen <- generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 1,
  max_index = 200000,
  shuffle = TRUE,
  step = step, 
  batch_size = batch_size
)

val_gen = generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 200001,
  max_index = 300000,
  step = step,
  batch_size = batch_size
)

test_gen <- generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 300001,
  max_index = NULL,
  step = step,
  batch_size = batch_size
)

val_steps <- (300000 - 200001 - lookback) / batch_size

test_steps <- (nrow(data) - 300001 - lookback) / batch_size
```

**A common sense, non-machine learning baseline**
```{r}
# compute the common-sense baselaine MAE
evaluate_naive_method <- function() {
  batch_maes <- c()
  for (step in 1:val_steps) {
    c(samples, targets) %<-% val_gen()
    preds <- samples[,dim(samples)[[2]],2]
    mae <- mean(abs(preds - targets))
    batch_maes <- c(batch_maes, mae)
  }
  print(mean(batch_maes))
}
```


**A basic machine learning approach**

```{r}
# train and evaluate a densely connected model
model <- keras_model_sequential() %>% 
  layer_flatten(input_shape = c(lookback / step, dim(data)[-1])) %>% 
  layer_dense(units = 32, activation = "relu") %>% 
  layer_dense(units = 1)

model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mae"
)

history <- model %>% fit_generator(
  train_gen,
  steps_per_epoch = 100,
  epochs = 5,
  validation_data = val_gen,
  validation_steps = val_steps
)
```


```{r}
plot(history)
```


**A first recurrent baseline**


```{r}
# train and evaluate a model with layer_gru
model <- keras_model_sequential() %>% 
  layer_gru(units = 32, input_shape = list(NULL, dim(data)[[-1]])) %>% 
  layer_dense(units = 1)

model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mae"
)

history <- model %>% fit_generator(
  train_gen,
  steps_per_epoch = 100,
  epochs = 5,
  validation_data = val_gen,
  validation_steps = val_steps
)
```



```{r}
plot(history)
```


**Using recurrent dropout to fight overfitting**

```{r}
# train and evaluate a drop-regularized GRU-based model
model <- keras_model_sequential() %>% 
  layer_gru(units = 32, dropout = 0.2, recurrent_dropout = 0.2,
            input_shape = list(NULL, dim(data)[[-1]])) %>% 
  layer_dense(units = 1)

model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mae"
)

history <- model %>% fit_generator(
  train_gen,
  steps_per_epoch = 100,
  epochs = 5,
  validation_data = val_gen,
  validation_steps = val_steps
)
```

```{r}
plot(history)
```


**Stacking recurrent layers**
 

```{r}
# train and evaluate a dropout-regularized, stacked GRU model
model <- keras_model_sequential() %>% 
  layer_gru(units = 32, 
            dropout = 0.1, 
            recurrent_dropout = 0.5,
            return_sequences = TRUE,
            input_shape = list(NULL, dim(data)[[-1]])) %>% 
  layer_gru(units = 64, activation = "relu",
            dropout = 0.1,
            recurrent_dropout = 0.5) %>% 
  layer_dense(units = 1)

model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mae"
)

history <- model %>% fit_generator(
  train_gen,
  steps_per_epoch = 100,
  epochs = 5,
  validation_data = val_gen,
  validation_steps = val_steps
)
```



```{r}
plot(history)
```


**Using bidirectional RNNs**

```{r}
reverse_order_generator <- function( data, lookback, delay, min_index, max_index,shuffle = FALSE, batch_size = 128, step = 6) 
  {
  if (is.null(max_index))
    max_index <- nrow(data) - delay - 1
  i <- min_index + lookback
  function() {
    if (shuffle) {
      rows <- sample(c((min_index+lookback):max_index), size = batch_size)
    } else {
      if (i + batch_size >= max_index)
        i <<- min_index + lookback
      rows <- c(i:min(i+batch_size, max_index))
      i <<- i + length(rows)
    }
    
    samples <- array(0, dim = c(length(rows), 
                                lookback / step,
                                dim(data)[[-1]]))
    targets <- array(0, dim = c(length(rows)))
                     
    for (j in 1:length(rows)) {
      indices <- seq(rows[[j]] - lookback, rows[[j]], 
                     length.out = dim(samples)[[2]])
      samples[j,,] <- data[indices,]
      targets[[j]] <- data[rows[[j]] + delay,2]
    }            
    
    list(samples[,ncol(samples):1,], targets)
  }
}

train_gen_reverse <- reverse_order_generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 1,
  max_index = 200000,
  shuffle = TRUE,
  step = step, 
  batch_size = batch_size
)

val_gen_reverse = reverse_order_generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 200001,
  max_index = 300000,
  step = step,
  batch_size = batch_size
)
```

```{r}
model <- keras_model_sequential() %>% 
  layer_gru(units = 32, input_shape = list(NULL, dim(data)[[-1]])) %>% 
  layer_dense(units = 1)

model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mae"
)

history <- model %>% fit_generator(
  train_gen_reverse,
  steps_per_epoch = 100,
  epochs = 5,
  validation_data = val_gen_reverse,
  validation_steps = val_steps
)
```

```{r}
plot(history)
```


```{r}
# train and evaluate and LSTM using reversed sequence
max_features <- 10000  # Number of words to consider as features
maxlen <- 500          # Cut texts after this number of words 
                      

# Load data
imdb <- dataset_imdb(num_words = max_features)
c(c(x_train, y_train), c(x_test, y_test)) %<-% imdb

# Reverse sequences
x_train <- lapply(x_train, rev) 
x_test <- lapply(x_test, rev) 

# Pad sequences
x_train <- pad_sequences(x_train, maxlen = maxlen)
x_test <- pad_sequences(x_test, maxlen = maxlen)

model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_features, output_dim = 128) %>% 
  layer_lstm(units = 32) %>% 
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)
  
history <- model %>% fit(
  x_train, y_train,
  epochs = 5,
  batch_size = 128,
  validation_split = 0.2
)
```

```{r}
plot(history)
```


```{r}
k_clear_session()
```

```{r}
# train and evaluate a bidirectional LSTM
model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_features, output_dim = 32) %>% 
  bidirectional(
    layer_lstm(units = 32)
  ) %>% 
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history <- model %>% fit(
  x_train, y_train,
  epochs = 10,
  batch_size = 128,
  validation_split = 0.2
)
```


```{r}
#train a bidirectional GRU
model <- keras_model_sequential() %>% 
  bidirectional(
    layer_gru(units = 32), input_shape = list(NULL, dim(data)[[-1]])
  ) %>% 
  layer_dense(units = 1)

model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mae"
)

history <- model %>% fit_generator(
  train_gen,
  steps_per_epoch = 200,
  epochs = 5,
  validation_data = val_gen,
  validation_steps = val_steps
)
```




**Sequence processing with convnets**

**Implementing a 1D convnet**
```{r}
# preparing the IMDB data
max_features <- 10000 
max_len <- 500
cat("Loading data...\n")
imdb <- dataset_imdb(num_words = max_features) 
c(c(x_train, y_train), c(x_test, y_test)) %<-% imdb 
cat(length(x_train), "train sequences\n") 
cat(length(x_test), "test sequences")
cat("Pad sequences (samples x time)\n")
x_train <- pad_sequences(x_train, maxlen = max_len)
x_test <- pad_sequences(x_test, maxlen = max_len) 
cat("x_train shape:", dim(x_train), "\n") 
cat("x_test shape:", dim(x_test), "\n")
```


```{r}
#train and evaluate a simple 1D convnet on the IMDB data
model <- keras_model_sequential() %>% layer_embedding(input_dim = max_features, output_dim = 128,
  input_length = max_len) %>%
  layer_conv_1d(filters = 32, kernel_size = 7, activation = "relu") %>%
  layer_max_pooling_1d(pool_size = 5) %>%
  layer_conv_1d(filters = 32, kernel_size = 7, activation = "relu") %>%
  layer_global_max_pooling_1d() %>%
  layer_dense(units = 1)

summary(model)

model %>% compile(
optimizer = optimizer_rmsprop(lr = 1e-4), loss = "binary_crossentropy",
metrics = c("acc")
)
history <- model %>% fit( x_train, y_train, epochs = 5,
batch_size = 128, validation_split = 0.2
)
```

```{r}
plot(history)
```


**Combining CNNs and RNNs to process long sequence**
```{r}
# train and evaluate a simple 1D convnet on the Jena data
model <- keras_model_sequential() %>%
layer_conv_1d(filters = 32, kernel_size = 5, activation = "relu",
input_shape = list(NULL, dim(data)[[-1]])) %>% layer_max_pooling_1d(pool_size = 3) %>%
  layer_conv_1d(filters = 32, kernel_size = 5, activation = "relu") %>%
  layer_max_pooling_1d(pool_size = 3) %>%
  layer_conv_1d(filters = 32, kernel_size = 5, activation = "relu") %>%
  layer_global_max_pooling_1d() %>%
  layer_dense(units = 1)
model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mae"
)
history <- model %>% fit_generator( train_gen,
steps_per_epoch = 100,
epochs = 5,
  validation_data = val_gen,
  validation_steps = val_steps
)
```

```{r}
plot(history)
```


```{r}
# prepare higher-resolution data generators for the Jena dataset
step <- 3 
lookback <- 720 
delay <- 144
train_gen <- generator( data,
  lookback = lookback,
  delay = delay,
  min_index = 1,
  max_index = 200000,
  shuffle = TRUE,
step = step )
val_gen <- generator( data,
  lookback = lookback,
  delay = delay,
  min_index = 200001,
  max_index = 300000,
  step = step
)
test_gen <- generator(data,
  lookback = lookback,
  delay = delay,
  min_index = 300001,
  max_index = NULL,
  step = step
)
val_steps <- (300000 - 200001 - lookback) / 128 
test_steps <- (nrow(data) - 300001 - lookback) / 128
```

```{r}
# model combining a 1D convulutional base and a GRU layer
model <- keras_model_sequential() %>%
layer_conv_1d(filters = 32, kernel_size = 5, activation = "relu",
input_shape = list(NULL, dim(data)[[-1]])) %>% layer_max_pooling_1d(pool_size = 3) %>%
layer_conv_1d(filters = 32, kernel_size = 5, activation = "relu") %>% layer_gru(units = 32, dropout = 0.1, recurrent_dropout = 0.5) %>% layer_dense(units = 1)
summary(model)
model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mae"
)
history <- model %>% fit_generator( train_gen,
steps_per_epoch = 100,
epochs = 5,
  validation_data = val_gen,
  validation_steps = val_steps
)
```

```{r}
plot(history)
```
